<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>ManiDext Project Page</title>
<!-- Bootstrap -->
<link href="./css/bootstrap-4.0.0.css" rel="stylesheet">
</head>


<body>
<div id="page_container">
<header>
  <div class="jumbotron">
    <div class="container">
      <div class="row">
        <div class="col-12">
          <!-- <h5 class="text-center">Anonymous website</h5> -->
          <h5 class="text-center">Arxiv 2024</h5>
          <h2 class="text-center">ManiDext: Hand-Object Manipulation Synthesis via Continuous Correspondence Embeddings and Residual-Guided Diffusion
          <!-- </h1> -->
          <hr>
          <p class="text-center">&nbsp;</p>
          <!-- <h6 class="text-center">Anonymous Submission</h6> -->
            <h6 class="text-center"> Jiajun Zhang<sup>1</sup>, 
            <a href="https://zhangyux15.github.io/">Yuxiang Zhang<sup>2</a>, 
            <a href="https://anl13.github.io/">Liang An<sup>2</a>,
            <a href="https://dw1010.github.io/">Mengcheng Li<sup>2</a>,
            <a href="https://zhanghongwen.cn">Hongwen Zhang<sup>3</a>,  
            <a href="https://see.bupt.edu.cn/info/1049/2056.htm">Zonghai Hu <sup>1</a>,
            <a href="https://liuyebin.com">Yebin Liu<sup>2</a></h6>
          
            <p class="text-center"><sup>1</sup>Beijing University of Posts and Telecommunications<br/> 
              <sup>2</sup>Tsinghua University<br/>
              <sup>3</sup>Beijing Normal University<br/>
              </p>
        </div>
      </div>
    </div>
  </div>
</header>

<section>
  <div class="container">
    <h2>&nbsp;</h2>
    <div class="row">
      <div class="col-lg-14 col-md-14 col-sm-14 text-center offset-xl-0 col-xl-12"> <img src="assets/teaser.png"
              width="950" alt="" />
      <br>
        <p class="text-justify"> </p>
        <p class="text-justify"> We present ManiDext, which takes a sequence of object motions as input and generates dexterous bimanual hand manipulations. 
        The motions are depicted through snapshots at several key frames. 
        Our hierarchical, diffusion-based pipeline first generates contact probability maps and continuous correspondence maps on the object's surface. 
        These contact details then guide the subsequent stage of hand pose generation.</p>
    </div>
  </div>
  <hr>

  <div class="container">
    <p>&nbsp;</p>
    <div class="row">
      <div class="col-lg-12 col-md-12 col-sm-12 col-xl-12 text-center">
        <h2>Abstract</h2>
      </div>
    </div>
  </div>

  <div class="container">
    <div class="row">
      <div class="col-lg-12 col-md-12 col-sm-12 text-center  offset-xl-0 col-xl-12">
        <p class="text-left"><em>
          Dynamic and dexterous manipulation of objects presents a complex challenge, requiring the synchronization of hand motions with the trajectories of objects to achieve seamless 
          and physically plausible interactions.
          In this work, we introduce ManiDext, a unified hierarchical diffusion-based framework for generating hand manipulation and grasp poses based on 3D object trajectories. 
          Our key insight is that accurately modeling the contact correspondences between objects and hands during interactions is crucial.
          Therefore, we propose a continuous correspondence embedding representation that specifies detailed hand correspondences at the vertex level between the object and the hand.
          This embedding is optimized directly on the hand mesh in a self-supervised manner, with the distance between embeddings reflecting the geodesic distance.
          Our framework first generates contact maps and correspondence embeddings on the object's surface.
          Based on these fine-grained correspondences, we introduce a novel approach that integrates the iterative refinement process into the diffusion process during the second stage of hand pose generation. 
          At each step of the denoising process, we incorporate the current hand pose residual as a refinement target into the network as a condition, guiding the network to correct inaccurate hand poses. 
          Introducing residuals into each denoising step inherently aligns with traditional optimization process, effectively merging generation and refinement into a single unified framework.
          Extensive experiments demonstrate that our approach can generate physically plausible and highly realistic motions for various tasks, 
          including single and bimanual hand grasping as well as manipulating both rigid and articulated objects. 
          </em></p>
        <p class="text-left">&nbsp;</p>
      </div>
    </div>
  </div>
  <hr>

  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h2>Overview</h2>
        <div class="col-lg-14 col-md-14 col-sm-14 text-center offset-xl-0 col-xl-12"> <img
            src="assets/pipeline.png" width="950" alt="" />
        </div>
        <br>
        <p class="text-justify"> </p>
        <p class="text-justify"> Method Overview. Given a sequence of object motion trajectory, we adopt a hierarchical diffusion-based framework to gradually generate the hand poses that manipulate the object. 
        First, we generate contact information on the object's surface, which includes a contact probability map and a continuous correspondence embedding map. 
        These information provides dense correspondence, allow us to compute the geometric residual error at each diffusion timestep T. 
        Subsequently, we use the generated contact information and the computed residual error as conditions to generate the manipulation hand poses.</p>
      </div>
    </div>
  </div>
  <hr>


  <div class="row">
    <div class="col-lg-12 mb-4 mt-2 text-center">
      <h2>Video</h2>
    </div>
  </div>

  <div class="col-lg-12 col-md-12 col-sm-12 col-xl-12 text-center">
    <video controls="controls" width="1024" height="576">
      <source src="https://github.com/jiajunzhang16/jiajunzhang16.github.io/raw/main/manidext/assets/ManiDext_supp_video.mp4" type="video/mp4">
    </video>
    <p>&nbsp;</p>
  </div>
   <hr>

  <div class="row">
    <div class="col-lg-12 mb-4 mt-2 text-center">
      <h2>Additional Results</h2>
    </div>
  </div>

  <div class="col-lg-12 col-md-12 col-sm-12 col-xl-12 text-center">
    <video controls="controls" width="1024" height="576">
      <source src="https://github.com/jiajunzhang16/jiajunzhang16.github.io/raw/main/manidext/assets/More_Results.mp4" type="video/mp4">
    </video>
    <div class="row">
      <div class="col-lg-12 col-md-12 col-sm-12 text-center offset-xl-0 col-xl-12">
        <p class="text-left"><em>
          More results of our method are shown in the video above. Each generated sequence contains 120 frames. 
 In addition, we include long-term generation results. To enable this, we retrain our framework with an inpainting objective: given the first 30 frames as additional conditions, the model predicts the following 90 frames. This enables the generation of overlapping segments that can be stitched into coherent long sequences.
        </em></p>
    </div>
    </div>
  </div>
   <hr>
    

  <div class="container">
      <div class="row ">
        <div class="col-12">
            <h3>Citation</h3>
                <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
                <code>@misc{zhang2024manidext,
                  title={ManiDext: Hand-Object Manipulation Synthesis via Continuous Correspondence Embeddings and Residual-Guided Diffusion},
                  author={Jiajun Zhang and Yuxiang Zhang and Liang An and Mengcheng Li and Hongwen Zhang and Zonghai Hu and Yebin Liu},
                  eprint={2409.09300},
                  archivePrefix={arXiv},
                  primaryClass={cs.CV},
                  url={https://arxiv.org/abs/2409.09300}, 
                  year={2024}
                }</code></pre>
            <hr>
        </div>
      </div>
  </div>
    
        
</section>
  </div>

  <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
  <script src="./js/jquery-3.2.1.min.js"></script>
  <!-- Include all compiled plugins (below), or include individual files as needed -->
  <script src="./js/popper.min.js"></script>
  <script src="./js/bootstrap-4.0.0.js"></script>
  <style>
    .myimg {
      vertical-align: top;
    }
  </style>
</body>

</html>



  

